{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep learning project 8DM20 CSMIA</h1>\n",
    "\n",
    "<h4>Group members:</h4>\n",
    "O. Akdag - 0842508 <br>\n",
    "T.P.A. Beishuizen - 0791613 <br>\n",
    "A.S.A. Eskelinen - 1224333 <br>\n",
    "J.H.A. Migchielsen - 0495058 <br>\n",
    "L. van den Wildenberg - 0844697 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import all used packages (unused packages are commented out so far)\n",
    "import os\n",
    "from PIL import Image as PIL_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from sklearn.feature_extraction import image as sklearn_image\n",
    "#matplotlib inline\n",
    "import theano\n",
    "import lasagne\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before every thing can be done, at first the data images have to be read and be made in useable data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The function that loads the data\n",
    "def loadData(data_set = 'test', image = '1st_manual'):\n",
    "    \n",
    "    # Check for the correct input\n",
    "    if data_set != 'test' and data_set != 'training':\n",
    "        raise Exception('Not the right data_set file')\n",
    "    if image != '1st_manual' and image != '2nd_manual' and image != 'images' and image != 'mask':\n",
    "        raise Exception('Not the right image file')\n",
    "    if data_set == 'training' and image == '2nd_manual':\n",
    "        raise Exception('File not available')\n",
    "    \n",
    "    # Project and image path\n",
    "    project_path = os.getcwd()\n",
    "    images_path = project_path +  '/8DM20_image_dataset/' + data_set + '/' + image + '/'\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    #Open image for image (20 in total for each of them)\n",
    "    for i in range(1, 21):\n",
    "        \n",
    "        # Find correct image number\n",
    "        image_nr = str(i)\n",
    "        if data_set == 'training':\n",
    "            image_nr = str(20 + i)\n",
    "        elif len(image_nr) == 1:\n",
    "            image_nr = '0' + image_nr\n",
    "            \n",
    "        # Specify path for this image\n",
    "        if image == '1st_manual':\n",
    "            image_path = images_path + image_nr + '_manual1.gif'\n",
    "        elif image == '2nd_manual':\n",
    "            image_path = images_path + image_nr + '_manual2.gif'\n",
    "        elif image == 'images':\n",
    "            image_path = images_path + image_nr + '_' + data_set + '.tif'\n",
    "        elif image == 'mask':\n",
    "            image_path = images_path + image_nr + '_' + data_set + '_mask.gif'\n",
    "        \n",
    "        # Open and append the image to the image list\n",
    "        images.append(PIL_image.open(image_path))\n",
    "        \n",
    "    return images\n",
    "\n",
    "#The function that converts the channels in the images from RGB to gray\n",
    "#and makes matrices from the images\n",
    "def convertImageToMatrix(images):\n",
    "    \n",
    "    image_matrices = []\n",
    "    \n",
    "    for image in images:\n",
    "        image_matrix = np.asarray(image.convert('L'))\n",
    "        image_matrices.append(image_matrix)\n",
    "        \n",
    "    return image_matrices\n",
    "\n",
    "#The function that prepares the image matrices to the data used for machine learning\n",
    "def prepareMachineLearningData(image_matrix, output_matrix, mask_matrix, kernel_size, mask_removal = 'pixel'):\n",
    "    #Check if correct input\n",
    "    if not (isinstance(image_matrix, np.ndarray) and \n",
    "            isinstance(output_matrix, np.ndarray) and \n",
    "            isinstance(mask_matrix, np.ndarray)):\n",
    "        raise Exception(\"Not all input matrices are numpy matrices\")\n",
    "    \n",
    "    if mask_removal != 'pixel' and mask_removal != 'patch':\n",
    "        raise Exception(\"Unknown mask data removal type\")\n",
    "    \n",
    "    if not (image_matrix.shape == output_matrix.shape == mask_matrix.shape):\n",
    "        raise Exception(\"The images are not the same size\")\n",
    "    \n",
    "    if np.unique(mask_matrix).shape[0] != 2:\n",
    "        raise Exception(\"The mask matrix does not consist of only 2 values\")\n",
    "    \n",
    "    #Creates a matrix with all possible patches\n",
    "    all_image_patches = sklearn_image.extract_patches_2d(image_matrix,(kernel_size,kernel_size))\n",
    "    all_image_patches = np.expand_dims(all_image_patches, axis=1)\n",
    "    \n",
    "    if kernel_size % 2 != 0:\n",
    "         # Creates an array with all output\n",
    "        mat_red = (kernel_size - 1) / 2\n",
    "        reduced_output_matrix = output_matrix[ mat_red : -  mat_red,  mat_red : -  mat_red]\n",
    "        complete_output_array = reduced_output_matrix.reshape(-1)\n",
    "\n",
    "        new_mask_matrix = mask_matrix.copy()\n",
    "        \n",
    "        # Makes some additional mask values zero on the edge of the mask\n",
    "        if mask_removal == 'patch':\n",
    "            for i in range(mat_red, mask_matrix.shape[0] -  mat_red + 1):\n",
    "                for j in range(mat_red, mask_matrix.shape[1] -  mat_red + 1):\n",
    "                    if 0 in mask_matrix[i - mat_red : i + mat_red + 1, j - mat_red: j + mat_red + 1]:\n",
    "                        new_mask_matrix[i,j] = 0;\n",
    "        \n",
    "        # Creates an array with all mask locations\n",
    "        reduced_mask_matrix = new_mask_matrix[ mat_red : -  mat_red, mat_red : -  mat_red]\n",
    "        mask_array = reduced_mask_matrix.reshape(-1)\n",
    "    \n",
    "    else:\n",
    "        # Creates an array with all output\n",
    "        mat_red = (kernel_size) / 2\n",
    "        reduced_output_matrix = output_matrix[mat_red - 1: -  mat_red,  mat_red - 1: -  mat_red]\n",
    "        complete_output_array = reduced_output_matrix.reshape(-1)\n",
    "\n",
    "        new_mask_matrix = mask_matrix.copy()\n",
    "        \n",
    "        # Makes some additional mask values zero on the edge of the mask\n",
    "        if mask_removal == 'patch':\n",
    "            for i in range(mat_red - 1, mask_matrix.shape[0] -  mat_red + 1):\n",
    "                for j in range(mat_red - 1, mask_matrix.shape[1] -  mat_red + 1):\n",
    "                    if 0 in mask_matrix[i - mat_red + 1 : i + mat_red + 1, j - mat_red + 1: j + mat_red + 1]:\n",
    "                        new_mask_matrix[i,j] = 0;\n",
    "                     \n",
    "        # Creates an array with all mask locations\n",
    "        reduced_mask_matrix = new_mask_matrix[mat_red - 1: - mat_red, mat_red - 1: - mat_red]\n",
    "        mask_array = reduced_mask_matrix.reshape(-1) \n",
    "\n",
    "    image_patches = []\n",
    "    output_array = []\n",
    "    \n",
    "    # Reduces the number of patches and output to only the mask values\n",
    "    for i in range(len(mask_array)):\n",
    "        if mask_array[i] != 0:\n",
    "            image_patches.append(all_image_patches[i,:,:])\n",
    "            output_array.append(complete_output_array[i])\n",
    "        \n",
    "    \n",
    "    # Return the image patches and the output array\n",
    "    return image_patches, output_array\n",
    "\n",
    "# Prepare multiple images at once\n",
    "def prepareMultipleImages(image_matrices, output_matrices, mask_matrices, kernel_size = 256, mask_removal = 'pixel'):\n",
    "    #Check if correct input\n",
    "    if not (isinstance(image_matrices, list) and \n",
    "            isinstance(output_matrices, list) and \n",
    "            isinstance(mask_matrices, list)):\n",
    "        raise Exception(\"Not all input matrices are numpy matrices\")\n",
    "    \n",
    "    if not (len(image_matrices) == len(output_matrices) == len(mask_matrices)):\n",
    "        raise Exception(\"The images are not the same size\")\n",
    "    \n",
    "    image_patches = [] \n",
    "    output_arrays = []\n",
    "    \n",
    "    # Finds the output data per image\n",
    "    for i in range(len(image_matrices)):\n",
    "        new_image_patches, new_output_array = prepareMachineLearningData(image_matrices[i], output_matrices[i], mask_matrices[i], \n",
    "                                                                         kernel_size = kernel_size, mask_removal = mask_removal)\n",
    "        image_patches.append(new_image_patches)\n",
    "        output_arrays.append(new_output_array)\n",
    "        \n",
    "        #Print progress for showing time consumption\n",
    "        print\"Progress: {} %\".format(100*(i+1)/len(image_matrices)),\n",
    "              \n",
    "    return image_patches, output_arrays\n",
    "\n",
    "def createVesselImage(output_array, mask_matrix, kernel_size):\n",
    "    #Check if input is correct\n",
    "    if not isinstance(output_array, list) or not isinstance(mask_matrix, np.ndarray) or not isinstance(kernel_size, int):\n",
    "        raise Exception(\"Not the right input variables\")\n",
    "    \n",
    "    #Create an output_matrix for the output array\n",
    "    #output_matrix = np.array(mask_matrix)\n",
    "    output_matrix = np.zeros(mask_matrix.shape)\n",
    "    output_loc = 0\n",
    "    \n",
    "    # Take into account that mask pixels too close to the border are lost due to inability to make patches\n",
    "    edge_corr = math.ceil(kernel_size / 2) - 1\n",
    "    \n",
    "    # Check every pixel within the mask for a vessel pixel\n",
    "    for i in range(mask_matrix.shape[0] - kernel_size + 1):\n",
    "        for j in range(mask_matrix.shape[1] - kernel_size + 1):\n",
    "            if mask_matrix[i + edge_corr, j + edge_corr] == 255:\n",
    "                output_matrix[i + edge_corr, j + edge_corr] = output_array[output_loc]\n",
    "                output_loc += 1\n",
    "                \n",
    "    return output_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images are loaded and immediately made into matrices for further computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All test image lists\n",
    "test_manual1_images = loadData('test', '1st_manual')\n",
    "test_manual2_images = loadData('test', '2nd_manual')\n",
    "test_raw_images = loadData('test', 'images')\n",
    "test_mask_images = loadData('test', 'mask')\n",
    "\n",
    "# Making matrices of the test images to work with\n",
    "test_manual1_matrices = convertImageToMatrix(test_manual1_images)\n",
    "test_manual2_matrices = convertImageToMatrix(test_manual2_images)\n",
    "test_raw_matrices = convertImageToMatrix(test_raw_images)\n",
    "test_mask_matrices = convertImageToMatrix(test_mask_images)\n",
    "\n",
    "# All training image lists\n",
    "training_manual1_images = loadData('training', '1st_manual')\n",
    "training_raw_images = loadData('training', 'images')\n",
    "training_mask_images = loadData('training', 'mask')\n",
    "\n",
    "# Making matrices of the training images to work with\n",
    "training_manual1_matrices = convertImageToMatrix(training_manual1_images)\n",
    "training_raw_matrices = convertImageToMatrix(training_raw_images)\n",
    "training_mask_matrices = convertImageToMatrix(training_mask_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrices are then used for further preprocessing to retrieve the suitable data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Choose the number of images\n",
    "nr_images_training = 2\n",
    "nr_images_test = 1\n",
    "\n",
    "# Prepares the data for machine learning: X = image_patches, y = output_array\n",
    "# Both are a list with the patches and output_arrays for multiple images (the number chosen before)\n",
    "image_patches, output_array = prepareMultipleImages(training_raw_matrices[0:nr_images_training], training_manual1_matrices[0:nr_images_training], \n",
    "                                                     training_mask_matrices[0:nr_images_training], 256, mask_removal = 'patch')\n",
    "test_image_patches, test_output_array = prepareMultipleImages(test_raw_matrices[0:nr_images_test], test_manual1_matrices[0:nr_images_test], \n",
    "                                                     test_mask_matrices[0:nr_images_test], 256, mask_removal = 'patch')\n",
    "print np.shape(image_patches)\n",
    "print np.shape(test_image_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is just to show how the data set is built up. There are patches of 32 x 32. These values either correspond to a vene pixel or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_number = 1\n",
    "pixel = 1 #35645\n",
    "\n",
    "\n",
    "np.asarray(image_patches[1]).shape\n",
    "\n",
    "plt.matshow(image_patches[image_number][pixel][0])\n",
    "plt.show()\n",
    "\n",
    "print(\"Should be vene pixel? \" + str(output_array[image_number - 1][pixel]) + \" (255 is yes, 0 is no)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For using machine learning. At first training and validation data has to be set up. This is almost done in the previous pre-processing part, however the data set still need to be assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_validation_set(image_patches, output_array):\n",
    "    all_train_patches = []\n",
    "    all_train_output = []\n",
    "\n",
    "    for i in range(nr_images_training):\n",
    "        if i <= (nr_images_training-1)/2 :\n",
    "            for j in range(len(image_patches[i])):\n",
    "                all_train_patches.append(image_patches[i][j])\n",
    "                all_train_output.append(output_array[i][j])\n",
    "        else:\n",
    "            valid_patches = image_patches[i]\n",
    "            valid_output = output_array[i]\n",
    "    \n",
    "    return all_train_patches, all_train_output, valid_patches, valid_output\n",
    "\n",
    "def hot_encoding(all_train_output, valid_output):\n",
    "    train_hot_output = np.zeros((len(all_train_output),2),dtype=np.int16)\n",
    "\n",
    "    # Make hot encoding training set\n",
    "    for i in range(len(train_hot_output)):\n",
    "        if all_train_output[i] == 0:\n",
    "            train_hot_output[i,0] = 1\n",
    "        else:\n",
    "            train_hot_output[i,1] = 1      \n",
    "\n",
    "    # Make hot encoding validation set\n",
    "    valid_hot_output = np.zeros((len(valid_output),2),dtype=np.int16)\n",
    "\n",
    "    for i in range(len(valid_hot_output)):\n",
    "        if valid_output[i] == 0:\n",
    "            valid_hot_output[i,0] = 1\n",
    "        else:\n",
    "            valid_hot_output[i,1] = 1\n",
    "    \n",
    "    return train_hot_output, valid_hot_output\n",
    "\n",
    "\n",
    "def test_set(test_image_patches, test_output_array):\n",
    "    all_test_patches = []\n",
    "    all_test_output_array = []\n",
    "    \n",
    "    for i in range(nr_images_test):\n",
    "        for j in range(len(test_image_patches[i])):\n",
    "            all_test_patches.append(test_image_patches[i][j])\n",
    "            all_test_output_array.append(test_output_array[i][j])\n",
    "                \n",
    "    return all_test_patches, all_test_output_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to first make the output array. This is done with hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_patches, all_train_output, valid_patches, valid_output = train_and_validation_set(image_patches, output_array)\n",
    "train_hot_output, valid_hot_output = hot_encoding(all_train_output, valid_output)\n",
    "\n",
    "all_test_patches, all_test_output_array = test_set(test_image_patches, test_output_array)\n",
    "\n",
    "print np.shape(all_train_patches)\n",
    "print np.shape(all_test_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the LeNet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildLeNet(X1):\n",
    "    inputlayer = lasagne.layers.InputLayer(shape=(None, 1, 32, 32),input_var=X1)    \n",
    "    print inputlayer.output_shape\n",
    "    \n",
    "    layer1 = lasagne.layers.Conv2DLayer(inputlayer, num_filters=64, filter_size=(3,3), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer2 = lasagne.layers.Conv2DLayer(layer1, num_filters=64, filter_size=(3,3), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer3 = lasagne.layers.MaxPool2DLayer(layer2, pool_size=(2, 2))\n",
    "    print layer2.output_shape \n",
    "    \n",
    "    layer4 = lasagne.layers.Conv2DLayer(layer3, num_filters=128, filter_size=(3,3), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer3.output_shape \n",
    "    \n",
    "    layer5 = lasagne.layers.Conv2DLayer(layer4, num_filters=128, filter_size=(3,3), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer6 = lasagne.layers.MaxPool2DLayer(layer5, pool_size=(2, 2))\n",
    "    print layer4.output_shape \n",
    "    \n",
    "    layer7 = lasagne.layers.Conv2DLayer(layer6, num_filters=256, filter_size=(3,3), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer8 = lasagne.layers.Conv2DLayer(layer7, num_filters=256, filter_size=(3,3), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer9 = lasagne.layers.Conv2DLayer(layer8, num_filters=256, filter_size=(1,1), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer10 = lasagne.layers.MaxPool2DLayer(layer9, pool_size=(2, 2))\n",
    "    print layer4.output_shape \n",
    "    \n",
    "    layer11 = lasagne.layers.Conv2DLayer(layer10, num_filters=512, filter_size=(3,3), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer12 = lasagne.layers.Conv2DLayer(layer11, num_filters=512, filter_size=(3,3), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer13 = lasagne.layers.Conv2DLayer(layer12, num_filters=512, filter_size=(1,1), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer14 = lasagne.layers.MaxPool2DLayer(layer13, pool_size=(2, 2))\n",
    "    print layer4.output_shape \n",
    "    \n",
    "    layer15 = lasagne.layers.Conv2DLayer(layer14, num_filters=512, filter_size=(3,3), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer16 = lasagne.layers.Conv2DLayer(layer15, num_filters=512, filter_size=(3,3), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer17 = lasagne.layers.Conv2DLayer(layer16, num_filters=512, filter_size=(1,1), nonlinearity=lasagne.nonlinearities.sigmoid, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer18 = lasagne.layers.MaxPool2DLayer(layer17, pool_size=(2, 2))\n",
    "    print layer4.output_shape \n",
    "    \n",
    "    layer19 = lasagne.layers.GlobalPoolLayer(layer18, pool_function=theano.tensor.mean)\n",
    "    print layer19.output_shape\n",
    "    \n",
    "    outputlayer = lasagne.layers.DenseLayer(layer19,num_units=2,nonlinearity=lasagne.nonlinearities.sigmoid)     \n",
    "    print outputlayer.output_shape \n",
    "    \n",
    "    return layer1, layer2, layer3, layer4, layer5, layer6, outputlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = theano.tensor.tensor4()\n",
    "Y = theano.tensor.matrix()\n",
    "layer1, layer2, layer3, layer4, layer5, layer6, layer7, layer8, layer8, layer10, layer11, layer12, layer13, layer14, layer15, layer16, layer17, layer18, layer19, outputlayer = buildLeNet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions for training, validating and testing the previously made LeNet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputtrain = lasagne.layers.get_output(outputlayer) #function that gets the output from the network defined before.\n",
    "trainloss = lasagne.objectives.categorical_crossentropy(outputtrain, Y).mean() #function that computes the mean crossentropy between the output and the real labels.\n",
    "params = lasagne.layers.get_all_params(outputlayer, trainable=True) #function that gets all the parameters (weights) in the network.\n",
    "updates = lasagne.updates.momentum(trainloss, params, learning_rate=0.001) #function that performs an update of the weights based on the loss.\n",
    "train = theano.function(inputs=[X, Y], outputs=trainloss, updates=updates, allow_input_downcast=True) #function that does all the above based on training samples X and real labels Y.\n",
    "\n",
    "validate = theano.function(inputs=[X, Y], outputs=trainloss, allow_input_downcast=True) #function that computes the loss without performing an update\n",
    "\n",
    "outputtest = lasagne.layers.get_output(outputlayer, deterministic=True) #function that gets the output from the network defined before.\n",
    "test = theano.function(inputs=[X], outputs=outputtest, allow_input_downcast=True) #function that gets the output based on input X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it is time to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training_the_network(all_train_output, valid_output, all_train_patches, valid_patches, minibatches = 250, minibatchsize = 100):\n",
    "\n",
    "    trainingsamples = np.arange(len(all_train_output)) #numbers from 0 until the number of samples\n",
    "    validsamples = np.arange(len(valid_output))\n",
    "\n",
    "    losslist = []\n",
    "    validlosslist = []\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in xrange(minibatches):\n",
    "        print(\"Currently at batch %d\" % i)\n",
    "\n",
    "        # Random train sample information. IMPORTANT: Use the hot encoded labels (that's the way the algorithm works)\n",
    "        random_train_samples = random.sample(trainingsamples, minibatchsize)\n",
    "        random_train_output = train_hot_output[random_train_samples]\n",
    "        random_train_patches = np.asarray(all_train_patches)[random_train_samples]\n",
    "\n",
    "        # Random validation sample information IMPORTANT: Use the hot encoded labels (that's the way the algorithm works)\n",
    "        random_valid_samples = random.sample(validsamples, minibatchsize)\n",
    "        random_valid_output = valid_hot_output[random_valid_samples]\n",
    "        random_valid_patches = np.asarray(valid_patches)[random_valid_samples]\n",
    "\n",
    "        print(random_train_output.shape)\n",
    "        print(random_train_patches.shape)\n",
    "\n",
    "        new_train_loss = train(random_train_patches, random_train_output)\n",
    "        losslist.append(new_train_loss)\n",
    "\n",
    "        new_valid_loss = validate(random_valid_patches, random_valid_output)\n",
    "        validlosslist.append(new_valid_loss)\n",
    "        #select random training en validation samples and perform training and validation steps here.\n",
    "\n",
    "    t1 = time.time()\n",
    "    print 'Training time: {} seconds'.format(t1-t0)\n",
    "    \n",
    "    return losslist, validlosslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losslist, validlosslist = training_the_network(all_train_output, valid_output, all_train_patches, valid_patches, minibatches = 250, minibatchsize = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losslist)\n",
    "plt.plot(validlosslist)\n",
    "plt.legend(['Training loss','Validation loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_matrix = createVesselImage(output_array[1], training_mask_matrices[1], 32)\n",
    "\n",
    "plt.matshow(output_matrix)\n",
    "plt.matshow(training_manual1_matrices[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Evaluation on the test set</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "testing = test(all_test_patches[0:20])\n",
    "test_set_predictions = np.argmax(testing, axis = 1)\n",
    "t1 = time.time()\n",
    "print 'Testing time: {} seconds'.format(t1-t0)\n",
    "\n",
    "print test_set_predictions\n",
    "print sum(test_set_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_test(all_test_output_array):\n",
    "    label_test_output = np.zeros(len(all_test_output_array),dtype=np.int16)\n",
    "\n",
    "    for i in range(len(all_test_output_array)):\n",
    "        if all_test_output_array[i] == 0:\n",
    "            label_test_output[i] = 0   \n",
    "        else:\n",
    "            label_test_output[i] = 1\n",
    "    \n",
    "    return label_test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_test_output = label_test(all_test_output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TP = np.sum(label_test_output[0:10] == test_set_predictions) \n",
    "print 'Accuracy: {}'.format(float(TP)/float(len(all_test_patches[0:10])))\n",
    "print TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>What has the network learned?</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputlayer1 = lasagne.layers.get_output(layer1) \n",
    "outputfeatures = theano.function(inputs=[X], outputs=outputlayer1, allow_input_downcast=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = outputfeatures(all_test_patches[1000:58621])\n",
    "print np.shape(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print features.shape\n",
    "for i in xrange(6):\n",
    "    plt.figure()\n",
    "    plt.imshow(features[1,i],cmap='gray_r',interpolation='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4> Visualising the filters </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = lasagne.layers.get_all_param_values(layer1)\n",
    "filters = weights[0]\n",
    "biases = weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print filters.shape\n",
    "print biases.shape\n",
    "for i in xrange(6):\n",
    "    plt.figure()\n",
    "    plt.imshow(filters[i,0],cmap='gray_r',interpolation='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Saving the network </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_weights(filename,network):\n",
    "    with open(filename, 'wb') as f:\n",
    "        cPickle.dump(lasagne.layers.get_all_param_values(network), f)\n",
    "        \n",
    "def load_weights(filename, network):\n",
    "    with open(filename, 'rb') as f:\n",
    "        lasagne.layers.set_all_param_values(network, cPickle.load(f))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
