{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep learning project 8DM20 CSMIA</h1>\n",
    "\n",
    "<h4>Group members:</h4>\n",
    "O. Akdag - 0842508 <br>\n",
    "T.P.A. Beishuizen - 0791613 <br>\n",
    "A.S.A. Eskelinen - 1224333 <br>\n",
    "J.H.A. Migchielsen - 0495058 <br>\n",
    "L. van den Wildenberg - 0844697 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atte\\Anaconda2\\lib\\site-packages\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# Import all used packages (unused packages are commented out so far)\n",
    "import os\n",
    "from PIL import Image as PIL_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from sklearn.feature_extraction import image as sklearn_image\n",
    "#matplotlib inline\n",
    "import theano\n",
    "import lasagne\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import cPickle\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Preprocessing</h4>\n",
    "\n",
    "Before every thing can be done, at first the data images have to be read and be made in useable data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The function that loads the data\n",
    "def loadData(data_set = 'test', image = '1st_manual'):\n",
    "    \n",
    "    # Check for the correct input\n",
    "    if data_set != 'test' and data_set != 'training':\n",
    "        raise Exception('Not the right data_set file')\n",
    "    if image != '1st_manual' and image != '2nd_manual' and image != 'images' and image != 'mask':\n",
    "        raise Exception('Not the right image file')\n",
    "    if data_set == 'training' and image == '2nd_manual':\n",
    "        raise Exception('File not available')\n",
    "    \n",
    "    # Project and image path\n",
    "    project_path = os.getcwd()\n",
    "    images_path = project_path +  '/8DM20_image_dataset/' + data_set + '/' + image + '/'\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    #Open image for image (20 in total for each of them)\n",
    "    for i in range(1, 21):\n",
    "        \n",
    "        # Find correct image number\n",
    "        image_nr = str(i)\n",
    "        if data_set == 'training':\n",
    "            image_nr = str(20 + i)\n",
    "        elif len(image_nr) == 1:\n",
    "            image_nr = '0' + image_nr\n",
    "            \n",
    "        # Specify path for this image\n",
    "        if image == '1st_manual':\n",
    "            image_path = images_path + image_nr + '_manual1.gif'\n",
    "        elif image == '2nd_manual':\n",
    "            image_path = images_path + image_nr + '_manual2.gif'\n",
    "        elif image == 'images':\n",
    "            image_path = images_path + image_nr + '_' + data_set + '.tif'\n",
    "        elif image == 'mask':\n",
    "            image_path = images_path + image_nr + '_' + data_set + '_mask.gif'\n",
    "        \n",
    "        # Open and append the image to the image list\n",
    "        images.append(PIL_image.open(image_path))\n",
    "        \n",
    "    return images\n",
    "\n",
    "#The function that converts the channels in the images from RGB to gray\n",
    "#and makes matrices from the images\n",
    "def convertImageToMatrix(images):\n",
    "    \n",
    "    image_matrices = []\n",
    "    \n",
    "    for image in images:\n",
    "        image_matrix = np.asarray(image.convert('RGB')) #L\n",
    "        image_matrix = image_matrix[:,:,1]\n",
    "        #print np.shape(image_matrix)\n",
    "        image_matrices.append(image_matrix)\n",
    "        \n",
    "    return image_matrices\n",
    "\n",
    "#The function that prepares the image matrices to the data used for machine learning\n",
    "def prepareMachineLearningData(image_matrix, output_matrix, mask_matrix, kernel_size, mask_removal = 'pixel'):\n",
    "    #Check if correct input\n",
    "    if not (isinstance(image_matrix, np.ndarray) and \n",
    "            isinstance(output_matrix, np.ndarray) and \n",
    "            isinstance(mask_matrix, np.ndarray)):\n",
    "        raise Exception(\"Not all input matrices are numpy matrices\")\n",
    "    \n",
    "    if mask_removal != 'pixel' and mask_removal != 'patch':\n",
    "        raise Exception(\"Unknown mask data removal type\")\n",
    "    \n",
    "    if not (image_matrix.shape == output_matrix.shape == mask_matrix.shape):\n",
    "        raise Exception(\"The images are not the same size\")\n",
    "    \n",
    "    if np.unique(mask_matrix).shape[0] != 2:\n",
    "        raise Exception(\"The mask matrix does not consist of only 2 values\")\n",
    "    \n",
    "    #Creates a matrix with all possible patches\n",
    "    all_image_patches = sklearn_image.extract_patches_2d(image_matrix,(kernel_size,kernel_size))\n",
    "    all_image_patches = np.expand_dims(all_image_patches, axis=1)\n",
    "    \n",
    "    if kernel_size % 2 != 0:\n",
    "         # Creates an array with all output\n",
    "        mat_red = (kernel_size - 1) / 2\n",
    "        reduced_output_matrix = output_matrix[ mat_red : -  mat_red,  mat_red : -  mat_red]\n",
    "        complete_output_array = reduced_output_matrix.reshape(-1)\n",
    "\n",
    "        new_mask_matrix = mask_matrix.copy()\n",
    "        \n",
    "        # Makes some additional mask values zero on the edge of the mask\n",
    "        if mask_removal == 'patch':\n",
    "            for i in range(mat_red, mask_matrix.shape[0] -  mat_red + 1):\n",
    "                for j in range(mat_red, mask_matrix.shape[1] -  mat_red + 1):\n",
    "                    if 0 in mask_matrix[i - mat_red : i + mat_red + 1, j - mat_red: j + mat_red + 1]:\n",
    "                        new_mask_matrix[i,j] = 0;\n",
    "        \n",
    "        # Creates an array with all mask locations\n",
    "        reduced_mask_matrix = new_mask_matrix[ mat_red : -  mat_red, mat_red : -  mat_red]\n",
    "        mask_array = reduced_mask_matrix.reshape(-1)\n",
    "    \n",
    "    else:\n",
    "        # Creates an array with all output\n",
    "        mat_red = (kernel_size) / 2\n",
    "        reduced_output_matrix = output_matrix[mat_red - 1: -  mat_red,  mat_red - 1: -  mat_red]\n",
    "        complete_output_array = reduced_output_matrix.reshape(-1)\n",
    "\n",
    "        new_mask_matrix = mask_matrix.copy()\n",
    "        \n",
    "        # Makes some additional mask values zero on the edge of the mask\n",
    "        if mask_removal == 'patch':\n",
    "            for i in range(mat_red - 1, mask_matrix.shape[0] -  mat_red - 1):\n",
    "                for j in range(mat_red - 1, mask_matrix.shape[1] -  mat_red - 1):\n",
    "                    if 0 in mask_matrix[i - mat_red + 1 : i + mat_red + 1, j - mat_red + 1: j + mat_red + 1]:\n",
    "                        new_mask_matrix[i,j] = 0;\n",
    "                     \n",
    "        # Creates an array with all mask locations\n",
    "        reduced_mask_matrix = new_mask_matrix[mat_red - 1: - mat_red, mat_red - 1: - mat_red]\n",
    "        mask_array = reduced_mask_matrix.reshape(-1) \n",
    "\n",
    "    image_patches = []\n",
    "    output_array = []    \n",
    "    \n",
    "    # Reduces the number of patches and output to only the mask values\n",
    "    for i in range(len(mask_array)):\n",
    "        if mask_array[i] != 0:\n",
    "            image_patches.append(all_image_patches[i,:,:])\n",
    "            output_array.append(complete_output_array[i])\n",
    "    \n",
    "    # Return the image patches and the output array\n",
    "    return image_patches, output_array\n",
    "\n",
    "# Prepare multiple images at once\n",
    "def prepareMultipleImages(image_matrices, output_matrices, mask_matrices, kernel_size = 25, mask_removal = 'pixel'):\n",
    "    #Check if correct input\n",
    "    if not (isinstance(image_matrices, list) and \n",
    "            isinstance(output_matrices, list) and \n",
    "            isinstance(mask_matrices, list)):\n",
    "        raise Exception(\"Not all input matrices are numpy matrices\")\n",
    "    \n",
    "    if not (len(image_matrices) == len(output_matrices) == len(mask_matrices)):\n",
    "        raise Exception(\"The images are not the same size\")\n",
    "    \n",
    "    image_patches = [] \n",
    "    output_arrays = []\n",
    "    \n",
    "    # Finds the output data per image\n",
    "    for i in range(len(image_matrices)):\n",
    "        new_image_patches, new_output_array = prepareMachineLearningData(image_matrices[i], output_matrices[i], mask_matrices[i], \n",
    "                                                                         kernel_size = kernel_size, mask_removal = mask_removal)\n",
    "        image_patches.append(new_image_patches)\n",
    "        output_arrays.append(new_output_array)\n",
    "        \n",
    "        #Print progress for showing time consumption\n",
    "        print\"Progress: {} %\".format(100*(i+1)/len(image_matrices)),\n",
    "              \n",
    "    return image_patches, output_arrays\n",
    "\n",
    "def createVesselImage(output_array, mask_matrix, kernel_size, mask_removal = \"pixel\"):\n",
    "    #Check if input is correct\n",
    "    if not isinstance(output_array, list) or not isinstance(mask_matrix, np.ndarray) or not isinstance(kernel_size, int):\n",
    "        raise Exception(\"Not the right input variables\")\n",
    "    \n",
    "    if mask_removal != \"pixel\" and mask_removal != 'patch':\n",
    "        raise Exception(\"Unknown mask removal type\")\n",
    "    \n",
    "    #Create an output_matrix for the output array\n",
    "    #output_matrix = np.array(mask_matrix)\n",
    "    output_matrix = np.zeros(mask_matrix.shape)\n",
    "    output_loc = 0\n",
    "    \n",
    "    # Take into account that mask pixels too close to the border are lost due to inability to make patches\n",
    "    edge_corr = int(math.ceil(kernel_size / 2) - 1)\n",
    "    \n",
    "    new_mask_matrix = mask_matrix.copy()\n",
    "    \n",
    "    # Makes some additional mask values zero on the edge of the mask\n",
    "    if mask_removal == 'patch':\n",
    "        for i in range(edge_corr, mask_matrix.shape[0] - edge_corr - 2):\n",
    "            for j in range(edge_corr, mask_matrix.shape[1] - edge_corr - 2):\n",
    "                if 0 in mask_matrix[i - edge_corr  : i + edge_corr + 2, j - edge_corr: j + edge_corr + 2]:\n",
    "                    new_mask_matrix[i,j] = 0;\n",
    "       \n",
    "    # Check every pixel within the mask for a vessel pixel\n",
    "    for i in range(mask_matrix.shape[0] - kernel_size + 1):\n",
    "        for j in range(mask_matrix.shape[1] - kernel_size + 1):\n",
    "            if new_mask_matrix[i + edge_corr, j + edge_corr] == 255:\n",
    "                output_matrix[i + edge_corr, j + edge_corr] = output_array[output_loc]\n",
    "                output_loc += 1\n",
    "                \n",
    "    return output_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images are loaded and immediately made into matrices for further computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All test image lists\n",
    "test_manual1_images = loadData('test', '1st_manual')\n",
    "test_manual2_images = loadData('test', '2nd_manual')\n",
    "test_raw_images = loadData('test', 'images')\n",
    "test_mask_images = loadData('test', 'mask')\n",
    "\n",
    "# Making matrices of the test images to work with\n",
    "test_manual1_matrices = convertImageToMatrix(test_manual1_images)\n",
    "test_manual2_matrices = convertImageToMatrix(test_manual2_images)\n",
    "test_raw_matrices = convertImageToMatrix(test_raw_images)\n",
    "test_mask_matrices = convertImageToMatrix(test_mask_images)\n",
    "\n",
    "# All training image lists\n",
    "training_manual1_images = loadData('training', '1st_manual')\n",
    "training_raw_images = loadData('training', 'images')\n",
    "training_mask_images = loadData('training', 'mask')\n",
    "\n",
    "# Making matrices of the training images to work with\n",
    "training_manual1_matrices = convertImageToMatrix(training_manual1_images)\n",
    "training_raw_matrices = convertImageToMatrix(training_raw_images)\n",
    "training_mask_matrices = convertImageToMatrix(training_mask_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_manual1_matrices\n",
    "#print np.shape(test_manual1_matrices)\n",
    "#plt.matshow(test_manual1_matrices[0])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrices are then used for further preprocessing to retrieve the suitable data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50 % Progress: 100 % Progress: 100 %\n"
     ]
    }
   ],
   "source": [
    "#Choose the number of images\n",
    "nr_images_training = 2\n",
    "nr_images_test = 1\n",
    "\n",
    "# Prepares the data for machine learning: X = image_patches, y = output_array\n",
    "# Both are a list with the patches and output_arrays for multiple images (the number chosen before)\n",
    "image_patches, output_array = prepareMultipleImages(training_raw_matrices[0:nr_images_training], training_manual1_matrices[0:nr_images_training], \n",
    "                                                     training_mask_matrices[0:nr_images_training], 32, mask_removal = 'patch')\n",
    "test_image_patches, test_output_array = prepareMultipleImages(test_raw_matrices[0:nr_images_test], test_manual1_matrices[0:nr_images_test], \n",
    "                                                     test_mask_matrices[0:nr_images_test], 32, mask_removal = 'patch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Algorithm setup</h4>\n",
    "The following algorithms are to show how the data set is built up. There are patches of 32x 32. These values either correspond to a vene pixel or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_validation_set(image_patches, output_array):\n",
    "    all_train_patches = []\n",
    "    all_train_output = []\n",
    "\n",
    "    for i in range(nr_images_training):\n",
    "        if i <= (nr_images_training-1)/2 :\n",
    "            for j in range(len(image_patches[i])):\n",
    "                all_train_patches.append(image_patches[i][j])\n",
    "                all_train_output.append(output_array[i][j])\n",
    "        else:\n",
    "            valid_patches = image_patches[i]\n",
    "            valid_output = output_array[i]\n",
    "    \n",
    "    return all_train_patches, all_train_output, valid_patches, valid_output\n",
    "\n",
    "def hot_encoding(all_train_output, valid_output):\n",
    "    train_hot_output = np.zeros((len(all_train_output),2),dtype=np.int16)\n",
    "\n",
    "    # Make hot encoding training set\n",
    "    for i in range(len(train_hot_output)):\n",
    "        if all_train_output[i] == 0:\n",
    "            train_hot_output[i,0] = 1\n",
    "        else:\n",
    "            train_hot_output[i,1] = 1      \n",
    "\n",
    "    # Make hot encoding validation set\n",
    "    valid_hot_output = np.zeros((len(valid_output),2),dtype=np.int16)\n",
    "\n",
    "    for i in range(len(valid_hot_output)):\n",
    "        if valid_output[i] == 0:\n",
    "            valid_hot_output[i,0] = 1\n",
    "        else:\n",
    "            valid_hot_output[i,1] = 1\n",
    "    \n",
    "    return train_hot_output, valid_hot_output\n",
    "\n",
    "\n",
    "def test_set(test_image_patches, test_output_array):\n",
    "    all_test_patches = []\n",
    "    all_test_output_array = []\n",
    "    \n",
    "    for i in range(nr_images_test):\n",
    "        for j in range(len(test_image_patches[i])):\n",
    "            all_test_patches.append(test_image_patches[i][j])\n",
    "            all_test_output_array.append(test_output_array[i][j])\n",
    "                \n",
    "    return all_test_patches, all_test_output_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to first make the output array. This is done with hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_patches, all_train_output, valid_patches, valid_output = train_and_validation_set(image_patches, output_array)\n",
    "train_hot_output, valid_hot_output = hot_encoding(all_train_output, valid_output)\n",
    "\n",
    "all_test_patches, all_test_output_array = test_set(test_image_patches, test_output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the LeNet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildLeNet(X1):\n",
    "    #rectify, softmax, sigmoid\n",
    "    inputlayer = lasagne.layers.InputLayer(shape=(None, 1, 32, 32),input_var=X1)    \n",
    "    print inputlayer.output_shape\n",
    "    \n",
    "    layer1 = lasagne.layers.Conv2DLayer(inputlayer, num_filters=6, filter_size=(5,5), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer2 = lasagne.layers.MaxPool2DLayer(layer1, pool_size=(2, 2))\n",
    "    print layer2.output_shape \n",
    "    \n",
    "    layer3 = lasagne.layers.Conv2DLayer(layer2, num_filters=16, filter_size=(5,5), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    print layer3.output_shape \n",
    "    \n",
    "    layer4 = lasagne.layers.MaxPool2DLayer(layer3, pool_size=(2, 2))\n",
    "    print layer4.output_shape \n",
    "    \n",
    "    layer4 = lasagne.layers.flatten(layer4)\n",
    "    print layer4.output_shape \n",
    "    \n",
    "    layer5 = lasagne.layers.DenseLayer(layer4,num_units=120,nonlinearity=lasagne.nonlinearities.rectify)    \n",
    "    print layer5.output_shape \n",
    "    \n",
    "    layer6 = lasagne.layers.DenseLayer(layer5,num_units=84,nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    print layer6.output_shape \n",
    "    \n",
    "    outputlayer = lasagne.layers.DenseLayer(layer6,num_units=2,nonlinearity=lasagne.nonlinearities.softmax)     \n",
    "    print outputlayer.output_shape \n",
    "    \n",
    "    return layer1, layer2, layer3, layer4, layer5, layer6, outputlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 32, 32)\n",
      "(None, 6, 28, 28)\n",
      "(None, 6, 14, 14)\n",
      "(None, 16, 10, 10)\n",
      "(None, 16, 5, 5)\n",
      "(None, 400)\n",
      "(None, 120)\n",
      "(None, 84)\n",
      "(None, 2)\n"
     ]
    }
   ],
   "source": [
    "X = theano.tensor.tensor4()\n",
    "Y = theano.tensor.matrix()\n",
    "layer1, layer2, layer3, layer4, layer5, layer6, outputlayer = buildLeNet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions for training, validation and testing purposes for the previously made LeNet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atte\\Anaconda2\\lib\\site-packages\\lasagne\\layers\\conv.py:489: UserWarning: The `image_shape` keyword argument to `tensor.nnet.conv2d` is deprecated, it has been renamed to `input_shape`.\n",
      "  border_mode=border_mode)\n"
     ]
    }
   ],
   "source": [
    "outputtrain = lasagne.layers.get_output(outputlayer) #function that gets the output from the network defined before.\n",
    "trainloss = lasagne.objectives.categorical_crossentropy(outputtrain, Y).mean() #function that computes the mean crossentropy between the output and the real labels.\n",
    "params = lasagne.layers.get_all_params(outputlayer, trainable=True) #function that gets all the parameters (weights) in the network.\n",
    "updates = lasagne.updates.momentum(trainloss, params, learning_rate=0.001) #function that performs an update of the weights based on the loss.\n",
    "train = theano.function(inputs=[X, Y], outputs=trainloss, updates=updates, allow_input_downcast=True) #function that does all the above based on training samples X and real labels Y.\n",
    "\n",
    "validate = theano.function(inputs=[X, Y], outputs=trainloss, allow_input_downcast=True) #function that computes the loss without performing an update\n",
    "\n",
    "outputtest = lasagne.layers.get_output(outputlayer, deterministic=True) #function that gets the output from the network defined before.\n",
    "test = theano.function(inputs=[X], outputs=outputtest, allow_input_downcast=True) #function that gets the output based on input X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training_the_network(all_train_output, valid_output, all_train_patches, valid_patches, minibatches = 250, minibatchsize = 100):\n",
    "\n",
    "    trainingsamples = np.arange(len(all_train_output)) #numbers from 0 until the number of samples\n",
    "    validsamples = np.arange(len(valid_output))\n",
    "\n",
    "    losslist = []\n",
    "    validlosslist = []\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in xrange(minibatches):\n",
    "        print(\"Currently at batch %d\" % i)\n",
    "\n",
    "        # Random train sample information. IMPORTANT: Use the hot encoded labels (that's the way the algorithm works)\n",
    "        random_train_patches, random_train_output = sampleBatches(all_train_patches, all_train_output, \n",
    "                                                                  batch_size = minibatchsize, distribution = 0.5)\n",
    "\n",
    "        # Random validation sample information IMPORTANT: Use the hot encoded labels (that's the way the algorithm works)\n",
    "        random_valid_patches, random_valid_output = sampleBatches(valid_patches, valid_output, \n",
    "                                                                  batch_size = minibatchsize, distribution = 0.5)\n",
    "\n",
    "        new_train_loss = train(random_train_patches, random_train_output)\n",
    "        losslist.append(new_train_loss)\n",
    "\n",
    "        new_valid_loss = validate(random_valid_patches, random_valid_output)\n",
    "        validlosslist.append(new_valid_loss)\n",
    "        #select random training en validation samples and perform training and validation steps here.\n",
    "\n",
    "    t1 = time.time()\n",
    "    print 'Training time: {} seconds'.format(t1-t0)\n",
    "    \n",
    "    return losslist, validlosslist\n",
    "\n",
    "\n",
    "# Creates bathces of vessel and non vessel images of a certain distribution\n",
    "def sampleBatches(input_patches, output_array, batch_size = 100, distribution = 0.5):\n",
    "    if len(input_patches) != len(output_array):\n",
    "        raise Exception(\"Length of input and output is different\")\n",
    "    \n",
    "    if distribution < 0 or distribution > 1:\n",
    "        raise Exception(\"Impossible distribution\")\n",
    "        \n",
    "    non_vessel_patches = []\n",
    "    vessel_patches = []\n",
    "    \n",
    "    #First create two lists. One with vessel patches and one without\n",
    "    for i in range(len(input_patches)):\n",
    "        if output_array[i] == 0:\n",
    "            non_vessel_patches.append(input_patches[i])\n",
    "            \n",
    "        else:\n",
    "            vessel_patches.append(input_patches[i])\n",
    "            \n",
    "    # Choose non vessel patches for in the batch\n",
    "    samples_non_vessel = np.arange(len(non_vessel_patches)) #numbers from 0 until the number of samples\n",
    "    random_non_vessel_samples = random.sample(samples_non_vessel, int(batch_size * distribution))\n",
    "    non_vessel_output = int(batch_size * distribution) * [[1, 0]] #all these were non-vessel\n",
    "    non_vessel_patches = np.asarray(non_vessel_patches)[random_non_vessel_samples]\n",
    "    \n",
    "    # Choose vessel patches for in the batch\n",
    "    samples_vessel = np.arange(len(vessel_patches)) #numbers from 0 until the number of samples\n",
    "    random_vessel_samples = random.sample(samples_vessel, int(batch_size * (1 - distribution)))\n",
    "    vessel_output = int(batch_size * (1 - distribution)) * [[0, 1]]\n",
    "    vessel_patches = np.asarray(vessel_patches)[random_vessel_samples]\n",
    "             \n",
    "    # Combine the batches    \n",
    "    batch_patches = np.append(non_vessel_patches, vessel_patches, axis = 0)\n",
    "    batch_output = np.append(non_vessel_output, vessel_output, axis = 0)\n",
    "                                          \n",
    "    return batch_patches, batch_output\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at batch 0\n",
      "Currently at batch 1\n",
      "Currently at batch 2\n",
      "Currently at batch 3\n",
      "Currently at batch 4\n",
      "Currently at batch 5\n",
      "Currently at batch 6\n",
      "Currently at batch 7\n",
      "Currently at batch 8\n",
      "Currently at batch 9\n",
      "Currently at batch 10\n",
      "Currently at batch 11\n",
      "Currently at batch 12\n",
      "Currently at batch 13\n",
      "Currently at batch 14\n",
      "Currently at batch 15\n",
      "Currently at batch 16\n",
      "Currently at batch 17\n",
      "Currently at batch 18\n",
      "Currently at batch 19\n",
      "Currently at batch 20\n",
      "Currently at batch 21\n",
      "Currently at batch 22\n",
      "Currently at batch 23\n",
      "Currently at batch 24\n",
      "Currently at batch 25\n",
      "Currently at batch 26\n",
      "Currently at batch 27\n",
      "Currently at batch 28\n",
      "Currently at batch 29\n",
      "Training time: 150.787000179 seconds\n"
     ]
    }
   ],
   "source": [
    "losslist, validlosslist = training_the_network(all_train_output, valid_output, all_train_patches, valid_patches, minibatches = 30, minibatchsize = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAK6iEhEKKCLiETE/wdSnk55aUtdbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf5e1128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192286L, 1L, 32L, 32L)\n",
      "[array(0.6933253615039198), array(0.6934071263766968), array(0.6935119855762508), array(0.6936324671990244), array(0.6937619222705541), array(0.6939024226835561), array(0.6940409562917842), array(0.6941863318899594), array(0.694341197938478)]\n"
     ]
    }
   ],
   "source": [
    "#plt.figure()\n",
    "#plt.plot(losslist)\n",
    "#plt.plot(validlosslist)\n",
    "#plt.legend(['Training loss','Validation loss'])\n",
    "#plt.show()\n",
    "print np.shape(all_test_patches)\n",
    "print losslist[-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.52453298  0.47546702]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.52420921  0.47579079]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]\n",
      " [ 0.5252745   0.4747255 ]]\n",
      "Testing time: 0.440999984741 seconds\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "testing = test(all_test_patches[100:600])\n",
    "print(testing)\n",
    "test_set_predictions = np.argmax(testing, axis = 1)\n",
    "t1 = time.time()\n",
    "print 'Testing time: {} seconds'.format(t1-t0)\n",
    "\n",
    "print test_set_predictions\n",
    "print sum(test_set_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#outputlayer1 = lasagne.layers.get_output(layer1) \n",
    "#outputfeatures = theano.function(inputs=[X], outputs=outputlayer1, allow_input_downcast=True) \n",
    "\n",
    "#features = outputfeatures(all_test_patches[1000:58621])\n",
    "#print np.shape(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print features.shape\n",
    "#for i in xrange(6):\n",
    "#    plt.figure()\n",
    "#    plt.imshow(features[1,i],cmap='gray_r',interpolation='none')\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.savetxt('test_set_predictions.txt', test_set_predictions)\n",
    "#np.savetxt('losslist.txt', losslist)\n",
    "#np.savetxt('validlosslist.txt', validlosslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_weights(filename,network):\n",
    "    with open(filename, 'wb') as f:\n",
    "        cPickle.dump(lasagne.layers.get_all_param_values(network), f)\n",
    "        cPickle.dump(test_set_predictions, f)\n",
    "        cPickle.dump(losslist, f)\n",
    "        cPickle.dump(validlosslist, f)\n",
    "        \n",
    "def load_weights(filename, network):\n",
    "    with open(filename, 'rb') as f:\n",
    "        lasagne.layers.set_all_param_values(network, cPickle.load(f))\n",
    "        \n",
    "filename = '/home/8dm20_3/Project CSMIA/Project1_weights.pkl' #'C:/Users/Atte/Desktop/Capita selecta/8DM20-CSMIA-group-3/Project/Project1_weights.pkl' \n",
    "network = outputlayer\n",
    "save_weights(filename, network) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
