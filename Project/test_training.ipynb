{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep learning project 8DM20 CSMIA</h1>\n",
    "\n",
    "<h4>Group members:</h4>\n",
    "O. Akdag - 0842508 <br>\n",
    "T.P.A. Beishuizen - 0791613 <br>\n",
    "A.S.A. Eskelinen - 1224333 <br>\n",
    "J.H.A. Migchielsen - 0495058 <br>\n",
    "L. van den Wildenberg - 0844697 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s137590\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# Import all used packages (unused packages are commented out so far)\n",
    "import os\n",
    "from PIL import Image as PIL_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from sklearn.feature_extraction import image as sklearn_image\n",
    "%matplotlib inline\n",
    "import theano\n",
    "import lasagne\n",
    "import time\n",
    "import random\n",
    "import cPickle\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before every thing can be done, at first the data images have to be read and be made in useable data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The function that loads the data\n",
    "def loadData(data_set = 'test', image = '1st_manual'):\n",
    "    \n",
    "    # Check for the correct input\n",
    "    if data_set != 'test' and data_set != 'training':\n",
    "        raise Exception('Not the right data_set file')\n",
    "    if image != '1st_manual' and image != '2nd_manual' and image != 'images' and image != 'mask':\n",
    "        raise Exception('Not the right image file')\n",
    "    if data_set == 'training' and image == '2nd_manual':\n",
    "        raise Exception('File not available')\n",
    "    \n",
    "    # Project and image path\n",
    "    project_path = os.getcwd()\n",
    "    images_path = project_path +  '/8DM20_image_dataset/' + data_set + '/' + image + '/'\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    #Open image for image (20 in total for each of them)\n",
    "    for i in range(1, 21):\n",
    "        \n",
    "        # Find correct image number\n",
    "        image_nr = str(i)\n",
    "        if data_set == 'training':\n",
    "            image_nr = str(20 + i)\n",
    "        elif len(image_nr) == 1:\n",
    "            image_nr = '0' + image_nr\n",
    "            \n",
    "        # Specify path for this image\n",
    "        if image == '1st_manual':\n",
    "            image_path = images_path + image_nr + '_manual1.gif'\n",
    "        elif image == '2nd_manual':\n",
    "            image_path = images_path + image_nr + '_manual2.gif'\n",
    "        elif image == 'images':\n",
    "            image_path = images_path + image_nr + '_' + data_set + '.tif'\n",
    "        elif image == 'mask':\n",
    "            image_path = images_path + image_nr + '_' + data_set + '_mask.gif'\n",
    "        \n",
    "        # Open and append the image to the image list\n",
    "        images.append(PIL_image.open(image_path))\n",
    "        \n",
    "    return images\n",
    "\n",
    "#The function that makes matrices from the images\n",
    "def convertImageToMatrix(images):\n",
    "    \n",
    "    image_matrices = []\n",
    "    \n",
    "    for image in images:\n",
    "        image_matrix = np.asarray(image.convert('L'))\n",
    "        image_matrices.append(image_matrix)\n",
    "        \n",
    "    return image_matrices\n",
    "\n",
    "#The function that prepares the image matrices to the data used for machine learning\n",
    "def prepareMachineLearningData(image_matrix, output_matrix, mask_matrix, kernel_size = 25):\n",
    "    #Check if correct input\n",
    "    if not (isinstance(image_matrix, np.ndarray) and \n",
    "            isinstance(output_matrix, np.ndarray) and \n",
    "            isinstance(mask_matrix, np.ndarray)):\n",
    "        raise Exception(\"Not all input matrices are numpy matrices\")\n",
    "    \n",
    "    if not (image_matrix.shape == output_matrix.shape == mask_matrix.shape):\n",
    "        raise Exception(\"The images are not the same size\")\n",
    "    \n",
    "    #if np.unique(output_matrix).shape[0] != 3:\n",
    "    #   raise Exception(\"The output matrix does not consist of only 3 values\")\n",
    "    \n",
    "    if np.unique(mask_matrix).shape[0] != 2:\n",
    "        raise Exception(\"The mask matrix does not consist of only 2 values\")\n",
    "        \n",
    "    if kernel_size % 2 != 1:\n",
    "        raise Exception(\"Not emplemented patches with even dimensions, yet\")\n",
    "    \n",
    "    #Creates a matrix with all possible patches\n",
    "    all_image_patches = sklearn_image.extract_patches_2d(image_matrix,(kernel_size,kernel_size))\n",
    "    \n",
    "    # Creates an array with all output\n",
    "    matrix_reduction = (kernel_size - 1) / 2\n",
    "    reduced_output_matrix = output_matrix[matrix_reduction : - matrix_reduction, matrix_reduction : - matrix_reduction]\n",
    "    complete_output_array = reduced_output_matrix.reshape(-1)\n",
    "\n",
    "    # Creates an array with all mask locations\n",
    "    reduced_mask_matrix = mask_matrix[matrix_reduction : - matrix_reduction, matrix_reduction : - matrix_reduction]\n",
    "    mask_array = reduced_mask_matrix.reshape(-1)\n",
    "    \n",
    "    image_patches = []\n",
    "    output_array = []\n",
    "    \n",
    "    # Reduces the number of patches and output to only the mask values\n",
    "    for i in range(len(mask_array)):\n",
    "        if mask_array[i] != 0:\n",
    "            image_patches.append(all_image_patches[i,:,:])\n",
    "            output_array.append(complete_output_array[i])\n",
    "\n",
    "    # Return the image patches and the output array\n",
    "    return image_patches, output_array\n",
    "\n",
    "# Prepare multiple images at once\n",
    "def prepareMultipleImages(image_matrices, output_matrices, mask_matrices, kernel_size = 25):\n",
    "    #Check if correct input\n",
    "    if not (isinstance(image_matrices, list) and \n",
    "            isinstance(output_matrices, list) and \n",
    "            isinstance(mask_matrices, list)):\n",
    "        raise Exception(\"Not all input matrices are numpy matrices\")\n",
    "    \n",
    "    if not (len(image_matrices) == len(output_matrices) == len(mask_matrices)):\n",
    "        raise Exception(\"The images are not the same size\")\n",
    "    \n",
    "    if kernel_size % 2 != 1:\n",
    "        raise Exception(\"Not emplemented patches with even dimensions, yet\")\n",
    "    \n",
    "    image_patches = [] \n",
    "    output_arrays = []\n",
    "    \n",
    "    # Finds the output data per image\n",
    "    for i in range(len(image_matrices)):\n",
    "        new_image_patches, new_output_array = prepareMachineLearningData(image_matrices[i], output_matrices[i], \n",
    "                                                                         mask_matrices[i], kernel_size = kernel_size)\n",
    "        image_patches.append(new_image_patches)\n",
    "        output_arrays.append(new_output_array)\n",
    "        \n",
    "        #Print progress for showing time consumption\n",
    "        print\"Progress: {} %\".format(100*(i+1)/len(image_matrices)),\n",
    "              \n",
    "    return image_patches, output_arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images are loaded and immediately made into matrices for further computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All test image lists\n",
    "test_manual1_images = loadData('test', '1st_manual')\n",
    "test_manual2_images = loadData('test', '2nd_manual')\n",
    "test_raw_images = loadData('test', 'images')\n",
    "test_mask_images = loadData('test', 'mask')\n",
    "\n",
    "# Making matrices of the test images to work with\n",
    "test_manual1_matrices = convertImageToMatrix(test_manual1_images)\n",
    "test_manual2_matrices = convertImageToMatrix(test_manual2_images)\n",
    "test_raw_matrices = convertImageToMatrix(test_raw_images)\n",
    "test_mask_matrices = convertImageToMatrix(test_mask_images)\n",
    "\n",
    "# All training image lists\n",
    "training_manual1_images = loadData('training', '1st_manual')\n",
    "training_raw_images = loadData('training', 'images')\n",
    "training_mask_images = loadData('training', 'mask')\n",
    "\n",
    "# Making matrices of the training images to work with\n",
    "training_manual1_matrices = convertImageToMatrix(training_manual1_images)\n",
    "training_raw_matrices = convertImageToMatrix(training_raw_images)\n",
    "training_mask_matrices = convertImageToMatrix(training_mask_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrices are then used for further preprocessing to retrieve the suitable data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50 % Progress: 100 %\n"
     ]
    }
   ],
   "source": [
    "#Choose the number of images\n",
    "nr_images = 2\n",
    "\n",
    "# Prepares the data for machine learning: X = image_patches, y = output_array\n",
    "# Both are a list with the patches and output_arrays for multiple images (the number chosen before)\n",
    "image_patches, output_array = prepareMultipleImages(training_raw_matrices[0:nr_images], training_manual1_matrices[0:nr_images], \n",
    "                                                     training_mask_matrices[0:nr_images], 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is just to show how the data set is built up. There are patches of 25x 25. These values either correspond to a vene pixel or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfYYAYIQQCAAKAgFAQSAAKAgEAMX/AfZeR75i4guMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bc59320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be vene pixel? 0 (255 is yes, 0 is no)\n"
     ]
    }
   ],
   "source": [
    "image_number = 1\n",
    "pixel = 289 #35645\n",
    "\n",
    "plt.matshow(image_patches[image_number][pixel])\n",
    "plt.show()\n",
    "\n",
    "print(\"Should be vene pixel? \" + str(output_array[image_number][pixel]) + \" (255 is yes, 0 is no)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildLeNet(X1):\n",
    "    inputlayer = lasagne.layers.InputLayer(shape=(None, 1, 25, 25),input_var=X1)    \n",
    "    #print inputlayer.output_shape\n",
    "    \n",
    "    layer1 = lasagne.layers.Conv2DLayer(inputlayer, num_filters=6, filter_size=(5,5), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    #print layer1.output_shape \n",
    "    \n",
    "    layer2 = lasagne.layers.MaxPool2DLayer(layer1, pool_size=(2, 2))\n",
    "    #print layer2.output_shape \n",
    "    \n",
    "    layer3 = lasagne.layers.Conv2DLayer(layer2, num_filters=16, filter_size=(5,5), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    #print layer3.output_shape \n",
    "    \n",
    "    layer4 = lasagne.layers.MaxPool2DLayer(layer3, pool_size=(2, 2))\n",
    "    #print layer4.output_shape \n",
    "    \n",
    "    layer4 = lasagne.layers.flatten(layer4)\n",
    "    #print layer4.output_shape \n",
    "    \n",
    "    layer5 = lasagne.layers.DenseLayer(layer4,num_units=120,nonlinearity=lasagne.nonlinearities.rectify)    \n",
    "    #print layer5.output_shape \n",
    "    \n",
    "    layer6 = lasagne.layers.DenseLayer(layer5,num_units=84,nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    #print layer6.output_shape \n",
    "    \n",
    "    outputlayer = lasagne.layers.DenseLayer(layer6,num_units=10,nonlinearity=lasagne.nonlinearities.softmax)     \n",
    "    #print outputlayer.output_shape \n",
    "    \n",
    "    return layer1, layer2, layer3, layer4, layer5, layer6, outputlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = theano.tensor.tensor4()\n",
    "Y = theano.tensor.matrix()\n",
    "layer1, layer2, layer3, layer4, layer5, layer6, outputlayer = buildLeNet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s137590\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\lasagne\\layers\\conv.py:489: UserWarning: The `image_shape` keyword argument to `tensor.nnet.conv2d` is deprecated, it has been renamed to `input_shape`.\n",
      "  border_mode=border_mode)\n"
     ]
    }
   ],
   "source": [
    "outputtrain = lasagne.layers.get_output(outputlayer) #function that gets the output from the network defined before.\n",
    "trainloss = lasagne.objectives.categorical_crossentropy(outputtrain, Y).mean() #function that computes the mean crossentropy between the output and the real labels.\n",
    "params = lasagne.layers.get_all_params(outputlayer, trainable=True) #function that gets all the parameters (weights) in the network.\n",
    "updates = lasagne.updates.momentum(trainloss, params, learning_rate=0.001) #function that performs an update of the weights based on the loss.\n",
    "train = theano.function(inputs=[X, Y], outputs=trainloss, updates=updates, allow_input_downcast=True) #function that does all the above based on training samples X and real labels Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate = theano.function(inputs=[X, Y], outputs=trainloss, allow_input_downcast=True) #function that computes the loss without performing an update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputtest = lasagne.layers.get_output(outputlayer, deterministic=True) #function that gets the output from the network defined before.\n",
    "test = theano.function(inputs=[X], outputs=outputtest, allow_input_downcast=True) #function that gets the output based on input X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-640e1870abd4>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-640e1870abd4>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    for i in xrange(2)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################# STRUCTURE ##############################################################\n",
    "\n",
    "for i in xrange(2)\n",
    "    trainingsamples = np.arange(len(image_patches[i])) # Amount of pixels for each image --> forloops\n",
    "    validsamples = np.arange(len(output_array[i]))     # Amount of pixels for each label (manual) --> forloops\n",
    "\n",
    "minibatches = 5000\n",
    "minibatchsize = 100 # bigger than the total amount of pixels for each image\n",
    "\n",
    "losslist = []\n",
    "validlosslist = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in xrange(minibatches):\n",
    "    #select random training en validation samples and perform training and validation steps here.    \n",
    "    minibatchsamples = random.sample(trainingsamples,minibatchsize)                \n",
    "\n",
    "    loss = train(image_patches[nr_images][minibatchsamples],output_array[minibatchsamples])\n",
    "    if (i+1)%100==0:\n",
    "        print 'Loss minibatch {}: {}'.format(i,loss)\n",
    "    losslist.append(loss)        \n",
    "    \n",
    "##########################################################################################################\n",
    "\n",
    "    validminibatchsamples = random.sample(validsamples,minibatchsize)           \n",
    "    validloss = validate(valid_set_images[validminibatchsamples],validlabels[validminibatchsamples])\n",
    "    if (i+1)%100==0:\n",
    "        print 'Loss validation minibatch {}: {}'.format(i,validloss)\n",
    "    validlosslist.append(validloss)\n",
    "\n",
    "t1 = time.time()\n",
    "print 'Training time: {} seconds'.format(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-45ba1a870104>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-45ba1a870104>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for i in range(2)\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for i in range(2)\n",
    "    trainingsamples = np.arange(len(image_patches[i])) # Amount of pixels for each image --> forloops\n",
    "    validsamples = np.arange(len(output_array[i]))     # Amount of pixels for each label (manual) --> forloops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losslist)\n",
    "plt.plot(validlosslist)\n",
    "plt.legend(['Training loss','Validation loss'])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
